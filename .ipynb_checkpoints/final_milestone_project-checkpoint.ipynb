{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd60e7d",
   "metadata": {},
   "source": [
    "# ChatGPT at 144p "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdd952",
   "metadata": {},
   "source": [
    " Since  **ChatGPT**  is going viral everywhere and people worrying about **ChatGPT** taking their jobs,I decided to be on macine's side to help take over jobs of humanity  **:)** .....and made this little program that takes in no.of.sentences as parameter and generates poetic text gramatically similar to  **Shakespeare**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7272939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense,Activation\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b728cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1115394/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Stores shakespeare sample text into a file \n",
    "\n",
    "filepath = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd8d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Reading/converting the file in binary mode\n",
    "then decoding it to utf-8 to make the file little space efficient for unicode characters\n",
    "and converting it into lower case to increase the accuray\n",
    "'''\n",
    "text = open(filepath,'rb').read().decode('utf-8').lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d18a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we will take a small portion of text  to take out unique characters and if that characters doesn't exist\n",
    "# in such a vast portion of text we will ignore that character to train our model\n",
    "\n",
    "smalltext = text[10000:80000]\n",
    "\n",
    "unique_characters = sorted(set(text)) # takes unique characters from smalltext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a27ef87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "character to index : \n",
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'a': 13, 'b': 14, 'c': 15, 'd': 16, 'e': 17, 'f': 18, 'g': 19, 'h': 20, 'i': 21, 'j': 22, 'k': 23, 'l': 24, 'm': 25, 'n': 26, 'o': 27, 'p': 28, 'q': 29, 'r': 30, 's': 31, 't': 32, 'u': 33, 'v': 34, 'w': 35, 'x': 36, 'y': 37, 'z': 38}\n",
      "Index to character : \n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'a', 14: 'b', 15: 'c', 16: 'd', 17: 'e', 18: 'f', 19: 'g', 20: 'h', 21: 'i', 22: 'j', 23: 'k', 24: 'l', 25: 'm', 26: 'n', 27: 'o', 28: 'p', 29: 'q', 30: 'r', 31: 's', 32: 't', 33: 'u', 34: 'v', 35: 'w', 36: 'x', 37: 'y', 38: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# We need to convert out text file into numerical representation to train our neural network\n",
    "# hence making a dictionary of all unique characters \n",
    "# It is like label encoding of all characters\n",
    "\n",
    "char_to_index = dict((i,c) for c,i in enumerate(unique_characters))\n",
    "\n",
    "print(\"character to index : \")\n",
    "print(char_to_index)\n",
    "\n",
    "\n",
    "# Also making a dictionary of index value to char for future reference\n",
    "\n",
    "print(\"Index to character : \")\n",
    "index_to_char = dict((c,i) for c,i in enumerate(unique_characters))\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b84c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create and empty list of sentences and nextcharacters\n",
    "# We will use sentences to load sentences into neural network and predict next character into nextcharacter\n",
    "# ex if sentence is 'how are yo' the predicted value should be 'u' to complete the sentence\n",
    "sentences = []\n",
    "next_characters = []\n",
    "\n",
    "# seq_length is used to define how much previous character should we need to predict the next character,we have taken it 40 \n",
    "# step size is set to 3 which is used\n",
    "# to jump from one sequence to other with step of 3 (similar to kernel size taught in convolutional neural netork in Devtownclass)\n",
    "seq_length = 40 \n",
    "step_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0174d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loop runs from start of sequence to last part of 'text' leaving 40 characters \n",
    "#which are then used to predict the next character with step size of 3\n",
    "\n",
    "for i in range(0,len(text)-seq_length,step_size):\n",
    "    sentences.append(text[i : i + seq_length])# This is our feature data\n",
    "    next_characters.append(text[i+seq_length])#This is our target data (this statement stores next single character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3ee370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We have one dimension for the sentences, we have seq_length as another dimension and length of unique characters as third dimension\n",
    "# Initially all value will be false since we have created an array with all zeros (np.zeroes)\n",
    "\n",
    "\n",
    "x = np.zeros((len(sentences),seq_length,len(unique_characters)), dtype = bool ) # features data \n",
    "y = np.zeros((len(sentences),len(unique_characters)) , dtype = bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37bdf25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop acts as kind of one-hot encoding of unique characters present in the sentence generated\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t,character in enumerate(sentence):\n",
    "        x[i,t, char_to_index[character]]= 1\n",
    "    y[i,char_to_index[next_characters[i]]] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0766a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dataset: [[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "Target dataset: [False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False]\n"
     ]
    }
   ],
   "source": [
    "# Printing sample of  feature and target dataset\n",
    "\n",
    "print(\"Feature dataset:\" ,x[0])\n",
    "print(\"Target dataset:\" , y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f892a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128,input_shape =(seq_length,len(unique_characters))))\n",
    "model.add(Dense(len(unique_characters)))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8946d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy' ,optimizer = RMSprop(learning_rate = 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffb2f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1453/1453 [==============================] - 288s 195ms/step - loss: 1.9519\n",
      "Epoch 2/4\n",
      "1453/1453 [==============================] - 277s 190ms/step - loss: 1.5912\n",
      "Epoch 3/4\n",
      "1453/1453 [==============================] - 278s 191ms/step - loss: 1.5036\n",
      "Epoch 4/4\n",
      "1453/1453 [==============================] - 280s 193ms/step - loss: 1.4589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x157bcea7f40>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y,batch_size = 256 ,epochs = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec854b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text_generator.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: text_generator.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('text_generator.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "254bdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('text_generator.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3376a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds,temperature = 1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) /temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds/np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a8e635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(length,temperature):\n",
    "    start_index = random.randint(0,len(text)- seq_length - 1)\n",
    "    generated = ''\n",
    "    sentence = text[start_index : start_index + seq_length]\n",
    "    generated +=sentence\n",
    "    for i in range(length):\n",
    "        x = np.zeros((1,seq_length,len(unique_characters)))\n",
    "        for t,character in enumerate(sentence):\n",
    "            x[0,t,char_to_index[character]] = 1\n",
    "        prediction = model.predict(x,verbose = 0)[0]\n",
    "        next_index = sample(prediction , temperature)\n",
    "        next_character = index_to_char[next_index]\n",
    "        \n",
    "        generated += next_character\n",
    "        sentence = sentence[1:] + next_character\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2e42eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", masters, let's home.\n",
      "i ever said we were the fair lords of my points,\n",
      "that i will not stand him from the seas of the body.\n",
      "\n",
      "lucio:\n",
      "the lord stand the commanded to the present of the command and stread.\n",
      "\n",
      "lucio:\n",
      "no, nor the command of the commanded with the body.\n",
      "\n",
      "coriolanus:\n",
      "why, i will not stand the adies of the command.\n",
      "\n",
      "coriolanus:\n",
      "wh\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(300,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecd11b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me you.\n",
      "\n",
      "york:\n",
      "what, will you go unto the seaved and will.\n",
      "\n",
      "coriolanus:\n",
      "your commend me in his father of some best.\n",
      "\n",
      "romeo:\n",
      "but the sension \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(100,0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fb4891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51480a47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f83d79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e032e5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2279ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e83952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
